{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c76c444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with a one-sided framework and OneSidedLinear interpolator.\n",
      "--- Generating and Adding RL Observations ---\n",
      "\n",
      "--- Distilling Likelihood (Unified Stochastic Score-Matching) ---\n",
      "Distillation Step 0, Unified Loss: 5745591808.0000\n",
      "Distillation Step 500, Unified Loss: 6463918592.0000\n",
      "Distillation complete.\n",
      "---\n",
      "\n",
      "--- Distilling Likelihood (Unified Stochastic Score-Matching) ---\n",
      "Distillation Step 0, Unified Loss: 10650065920.0000\n",
      "Distillation Step 500, Unified Loss: 5198866432.0000\n",
      "Distillation complete.\n",
      "---\n",
      "\n",
      "--- Distilling Likelihood (Unified Stochastic Score-Matching) ---\n",
      "Distillation Step 0, Unified Loss: 7336397312.0000\n",
      "Distillation Step 500, Unified Loss: 394143072.0000\n",
      "Distillation complete.\n",
      "---\n",
      "\n",
      "--- Distilling Likelihood (Unified Stochastic Score-Matching) ---\n",
      "Distillation Step 0, Unified Loss: 8002585600.0000\n",
      "Distillation Step 500, Unified Loss: 9978874880.0000\n",
      "Distillation complete.\n",
      "---\n",
      "\n",
      "--- Distilling Likelihood (Unified Stochastic Score-Matching) ---\n",
      "Distillation Step 0, Unified Loss: 32052383744.0000\n",
      "Distillation Step 500, Unified Loss: 11760459776.0000\n",
      "Distillation complete.\n",
      "---\n",
      "\n",
      "--- Distilling Likelihood (Unified Stochastic Score-Matching) ---\n",
      "Distillation Step 0, Unified Loss: 9513647104.0000\n",
      "Distillation Step 500, Unified Loss: 38925553664.0000\n",
      "Distillation complete.\n",
      "---\n",
      "\n",
      "--- Distilling Likelihood (Unified Stochastic Score-Matching) ---\n",
      "Distillation Step 0, Unified Loss: 11207447552.0000\n",
      "Distillation Step 500, Unified Loss: 20388986880.0000\n",
      "Distillation complete.\n",
      "---\n",
      "\n",
      "--- Distilling Likelihood (Unified Stochastic Score-Matching) ---\n",
      "Distillation Step 0, Unified Loss: 8585894400.0000\n",
      "Distillation Step 500, Unified Loss: 10510642176.0000\n",
      "Distillation complete.\n",
      "---\n",
      "\n",
      "--- Distilling Likelihood (Unified Stochastic Score-Matching) ---\n",
      "Distillation Step 0, Unified Loss: 7362456064.0000\n",
      "Distillation Step 500, Unified Loss: 30142267392.0000\n",
      "Distillation complete.\n",
      "---\n",
      "\n",
      "--- Distilling Likelihood (Unified Stochastic Score-Matching) ---\n",
      "Distillation Step 0, Unified Loss: 11569509376.0000\n",
      "Distillation Step 500, Unified Loss: 31885903872.0000\n",
      "Distillation complete.\n",
      "---\n",
      "\n",
      "--- Distilling Likelihood (Unified Stochastic Score-Matching) ---\n",
      "Distillation Step 0, Unified Loss: 9820944384.0000\n",
      "Distillation Step 500, Unified Loss: 11466345472.0000\n",
      "Distillation complete.\n",
      "---\n",
      "\n",
      "--- Distilling Likelihood (Unified Stochastic Score-Matching) ---\n",
      "Distillation Step 0, Unified Loss: 9475467264.0000\n",
      "Distillation Step 500, Unified Loss: 18562414592.0000\n",
      "Distillation complete.\n",
      "---\n",
      "\n",
      "--- Distilling Likelihood (Unified Stochastic Score-Matching) ---\n",
      "Distillation Step 0, Unified Loss: 24388818944.0000\n",
      "Distillation Step 500, Unified Loss: 30076168192.0000\n",
      "Distillation complete.\n",
      "---\n",
      "\n",
      "--- Distilling Likelihood (Unified Stochastic Score-Matching) ---\n",
      "Distillation Step 0, Unified Loss: 17130496000.0000\n",
      "Distillation Step 500, Unified Loss: 854242752.0000\n",
      "Distillation complete.\n",
      "---\n",
      "\n",
      "--- Distilling Likelihood (Unified Stochastic Score-Matching) ---\n",
      "Distillation Step 0, Unified Loss: 50018103296.0000\n",
      "Distillation Step 500, Unified Loss: 34666930176.0000\n",
      "Distillation complete.\n",
      "---\n",
      "\n",
      "--- Distilling Likelihood (Unified Stochastic Score-Matching) ---\n",
      "Distillation Step 0, Unified Loss: 11529454592.0000\n",
      "Distillation Step 500, Unified Loss: 10497193984.0000\n",
      "Distillation complete.\n",
      "---\n",
      "\n",
      "--- Distilling Likelihood (Unified Stochastic Score-Matching) ---\n",
      "Distillation Step 0, Unified Loss: 13214880768.0000\n",
      "Distillation Step 500, Unified Loss: 15253728256.0000\n",
      "Distillation complete.\n",
      "---\n",
      "\n",
      "--- Distilling Likelihood (Unified Stochastic Score-Matching) ---\n",
      "Distillation Step 0, Unified Loss: 14912978944.0000\n",
      "Distillation Step 500, Unified Loss: 12507274240.0000\n",
      "Distillation complete.\n",
      "---\n",
      "\n",
      "--- Distilling Likelihood (Unified Stochastic Score-Matching) ---\n",
      "Distillation Step 0, Unified Loss: 13675415552.0000\n",
      "Distillation Step 500, Unified Loss: 4325082624.0000\n",
      "Distillation complete.\n",
      "---\n",
      "\n",
      "--- Distilling Likelihood (Unified Stochastic Score-Matching) ---\n",
      "Distillation Step 0, Unified Loss: 16076646400.0000\n",
      "Distillation Step 500, Unified Loss: 18235129856.0000\n",
      "Distillation complete.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, value_and_grad, random\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from bayes.posterior import FlowBasedPosterior, PRNGKeyManager\n",
    "from sinterp.interpolants import OneSidedLinear\n",
    "\n",
    "# jax.config.update('jax_disable_jit', True)\n",
    "jax.config.update(\"jax_debug_nans\", True)\n",
    "\n",
    "# 1. Define the Ground-Truth RL Environment\n",
    "N_STATES = 4\n",
    "N_ACTIONS = 2\n",
    "REWARD_NOISE_STD = 0.1 # Fixed standard deviation for reward generation\n",
    "\n",
    "# Use a key for reproducible ground-truth generation\n",
    "key = random.PRNGKey(0)\n",
    "key, T_key, R_key = random.split(key, 3)\n",
    "\n",
    "# True Transition Matrix (S x A x S')\n",
    "# Random logits, then softmax over the last dimension (s') to ensure probabilities sum to 1.\n",
    "true_transition_logits = random.normal(T_key, (N_STATES, N_ACTIONS, N_STATES))\n",
    "true_transition_matrix = jax.nn.softmax(true_transition_logits, axis=-1)\n",
    "\n",
    "# True Reward Matrix (S x A)\n",
    "# Rewards are centered around these values.\n",
    "true_reward_matrix = random.uniform(R_key, (N_STATES, N_ACTIONS), minval=-5, maxval=5)\n",
    "\n",
    "\n",
    "# 2. Define Parameter Space and Likelihood Function\n",
    "REWARD_BOUND = 10.0\n",
    "# The total number of parameters to learn for the forward model\n",
    "PARAM_DIM = (N_STATES * N_ACTIONS * N_STATES) + (N_STATES * N_ACTIONS)\n",
    "# (transition params)      +      (reward params)\n",
    "\n",
    "@dataclass\n",
    "class RLTheta:\n",
    "    \"\"\"A structured container for the RL forward model parameters.\"\"\"\n",
    "    log_transition_matrix: jnp.ndarray # Shape: (S, A, S')\n",
    "    reward_matrix: jnp.ndarray         # Shape: (S, A)\n",
    "\n",
    "def vec_to_theta_rl(h: jnp.ndarray) -> RLTheta:\n",
    "    \"\"\"\n",
    "    Converts a flat vector `h` into a structured RLTheta object.\n",
    "    Ensures the transition matrix is properly normalized.\n",
    "    \"\"\"\n",
    "    # 1. Slice the flat vector `h` into its constituent parts.\n",
    "    c = 0\n",
    "    transition_params = h[c:c + N_STATES * N_ACTIONS * N_STATES].reshape(N_STATES, N_ACTIONS, N_STATES)\n",
    "    c += N_STATES * N_ACTIONS * N_STATES\n",
    "    \n",
    "    reward_params = h[c:].reshape(N_STATES, N_ACTIONS)\n",
    "\n",
    "    # 2. Ensure the transition matrix is valid using log_softmax.\n",
    "    # This is numerically stable and guarantees that for each (s, a),\n",
    "    # the probabilities over s' sum to 1.\n",
    "    log_transition_matrix = jax.nn.log_softmax(transition_params, axis=-1)\n",
    "    \n",
    "    # 3. The reward matrix\n",
    "    reward_matrix = REWARD_BOUND * jnp.tanh(reward_params)\n",
    "\n",
    "    return RLTheta(log_transition_matrix, reward_matrix)\n",
    "\n",
    "def forward_model_log_likelihood(theta: RLTheta, observation: jnp.ndarray):\n",
    "    \"\"\"\n",
    "    Calculates log p(s', r | s, a, theta) for a single observation.\n",
    "    observation is a vector [s, a, r, s'].\n",
    "    \"\"\"\n",
    "    s, a, r, s_prime = observation[0], observation[1], observation[2], observation[3]\n",
    "    \n",
    "    # Cast state/action to integers to use as indices\n",
    "    s, a, s_prime = s.astype(int), a.astype(int), s_prime.astype(int)\n",
    "\n",
    "    # Log-likelihood of the transition p(s' | s, a, theta)\n",
    "    log_p_transition = theta.log_transition_matrix[s, a, s_prime]\n",
    "    \n",
    "    # Log-likelihood of the reward p(r | s, a, theta)\n",
    "    # Assumes reward is Gaussian with mean R(s,a) and fixed variance.\n",
    "    expected_reward = theta.reward_matrix[s, a]\n",
    "    log_p_reward = jax.scipy.stats.norm.logpdf(r, loc=expected_reward, scale=REWARD_NOISE_STD)\n",
    "    \n",
    "    # Total log-likelihood is the sum (since log(A*B) = log(A) + log(B))\n",
    "    return log_p_transition + log_p_reward\n",
    "\n",
    "def build_total_log_likelihood_and_grad_rl(observations):\n",
    "    \"\"\"\n",
    "    Builds the function that computes the gradient of the total log-likelihood\n",
    "    with respect to the FLATTENED parameter vector `h`.\n",
    "    \"\"\"\n",
    "    y_data = jnp.array(observations)\n",
    "\n",
    "    def total_log_likelihood(h, y_data_batch):\n",
    "        \"\"\"Calculates sum of log p(y_i | h) for a batch of observations.\"\"\"\n",
    "        theta = vec_to_theta_rl(h)\n",
    "        \n",
    "        # Use vmap to efficiently calculate likelihood over the whole batch\n",
    "        log_likelihoods = vmap(partial(forward_model_log_likelihood, theta))(y_data_batch)\n",
    "        \n",
    "        return jnp.sum(log_likelihoods)\n",
    "\n",
    "    # Differentiate with respect to the flattened parameter vector `h`\n",
    "    total_log_likelihood_grad_fn = grad(total_log_likelihood, argnums=0)\n",
    "    \n",
    "    return total_log_likelihood_grad_fn, y_data\n",
    "\n",
    "# 3. Initialize the Posterior Model\n",
    "key_manager = PRNGKeyManager(seed=1)\n",
    "# interpolator = get_interp('OneSidedLinear') # Assuming this is available\n",
    "\n",
    "# Initialize the posterior over the PARAMETER space of the RL model\n",
    "posterior = FlowBasedPosterior(\n",
    "    build_total_log_likelihood_and_grad=build_total_log_likelihood_and_grad_rl,\n",
    "    dim=PARAM_DIM,\n",
    "    key_manager=key_manager,\n",
    "    interpolator=OneSidedLinear(),\n",
    "    distillation_threshold=100\n",
    ")\n",
    "\n",
    "# 4. Simulate and add observations to the model\n",
    "print(\"--- Generating and Adding RL Observations ---\")\n",
    "num_observations = 2000\n",
    "for _ in range(num_observations):\n",
    "    # Sample a random state and action\n",
    "    key, s_key, a_key, t_key, r_key = random.split(key, 5)\n",
    "    s = random.randint(s_key, (), 0, N_STATES)\n",
    "    a = random.randint(a_key, (), 0, N_ACTIONS)\n",
    "    \n",
    "    # Sample next state from the true transition model\n",
    "    s_prime = random.choice(t_key, N_STATES, p=true_transition_matrix[s, a])\n",
    "    \n",
    "    # Sample reward from the true reward model\n",
    "    r = random.normal(r_key) * REWARD_NOISE_STD + true_reward_matrix[s, a]\n",
    "    \n",
    "    # Add the observation tuple (as a jax array)\n",
    "    posterior.add_observation(jnp.array([s, a, r, s_prime]))\n",
    "\n",
    "# # 5. Find the MAP estimate of the parameters\n",
    "# h_map, final_log_prob = find_map_with_overparameterization(\n",
    "#     posterior=posterior,\n",
    "#     key_manager=key_manager,\n",
    "#     num_steps=5000,\n",
    "#     learning_rate=1e-3\n",
    "# )\n",
    "\n",
    "\n",
    "h_map = posterior.sample(key, (1,))[0]\n",
    "\n",
    "# 6. Verify the result against the true RL model parameters\n",
    "theta_map_rl = vec_to_theta_rl(h_map)\n",
    "\n",
    "print(\"\\n--- Verification of Learned RL Model ---\")\n",
    "# print(f\"Final Average Log-Likelihood: {final_log_prob:.4f}\")\n",
    "\n",
    "# To compare transition matrices, we exponentiate the learned log-probabilities\n",
    "found_transition_matrix = jnp.exp(theta_map_rl.log_transition_matrix)\n",
    "found_reward_matrix = theta_map_rl.reward_matrix\n",
    "\n",
    "print(\"\\n--- Comparing True vs. Found Transition Matrix ---\")\n",
    "print(\"Note: Values are p(s' | s, a). Should be close if data is sufficient.\")\n",
    "for s in range(N_STATES):\n",
    "    for a in range(N_ACTIONS):\n",
    "        print(f\"\\nState={s}, Action={a}\")\n",
    "        print(f\"  True:    {true_transition_matrix[s, a]}\")\n",
    "        print(f\"  Found:   {found_transition_matrix[s, a]}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Comparing True vs. Found Reward Matrix ---\")\n",
    "print(\"Note: Values are E[r | s, a]. Should be close.\")\n",
    "print(\"True:\\n\", true_reward_matrix)\n",
    "print(\"Found:\\n\", found_reward_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210ba708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verification of Learned RL Model ---\n",
      "\n",
      "--- Comparing True vs. Found Transition Matrix ---\n",
      "Note: Values are p(s' | s, a). Should be close if data is sufficient.\n",
      "\n",
      "State=0, Action=0\n",
      "  True:    [0.04047961 0.06079901 0.5718052  0.3269162 ]\n",
      "  Found:   [0.22874711 0.09776904 0.12090193 0.55258197]\n",
      "\n",
      "State=0, Action=1\n",
      "  True:    [0.19146848 0.12623502 0.13012268 0.5521739 ]\n",
      "  Found:   [0.18394393 0.12047469 0.12903614 0.56654525]\n",
      "\n",
      "State=1, Action=0\n",
      "  True:    [0.02222157 0.693535   0.06817152 0.21607192]\n",
      "  Found:   [0.20910555 0.01902924 0.71104014 0.06082499]\n",
      "\n",
      "State=1, Action=1\n",
      "  True:    [0.09101926 0.1581395  0.23025165 0.5205896 ]\n",
      "  Found:   [0.20085382 0.10000684 0.61061186 0.08852748]\n",
      "\n",
      "State=2, Action=0\n",
      "  True:    [0.17254084 0.5293649  0.21808279 0.08001144]\n",
      "  Found:   [0.05014689 0.40201223 0.4023899  0.14545095]\n",
      "\n",
      "State=2, Action=1\n",
      "  True:    [0.04666347 0.10220193 0.5334075  0.31772703]\n",
      "  Found:   [0.2986915  0.08569892 0.09943836 0.5161712 ]\n",
      "\n",
      "State=3, Action=0\n",
      "  True:    [0.03750316 0.06940839 0.2680248  0.62506366]\n",
      "  Found:   [0.21889886 0.07387051 0.14448789 0.56274277]\n",
      "\n",
      "State=3, Action=1\n",
      "  True:    [0.27520892 0.43734783 0.21668664 0.07075652]\n",
      "  Found:   [0.07737708 0.6938813  0.20074709 0.02799461]\n",
      "\n",
      "--- Comparing True vs. Found Reward Matrix ---\n",
      "Note: Values are E[r | s, a]. Should be close.\n",
      "True:\n",
      " [[ 4.024495    4.1229286 ]\n",
      " [-1.5895236  -2.799089  ]\n",
      " [ 1.483767    0.07535219]\n",
      " [ 2.1720433  -2.7435887 ]]\n",
      "Found:\n",
      " [[ 9.341488  -8.34072  ]\n",
      " [-8.07495    7.017116 ]\n",
      " [ 6.4865026 -0.6414547]\n",
      " [-8.598271   3.6848712]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c8613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
